{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LXw1gssjz9yj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# 코드 다시 돌리기 위한 seed 고정\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel(nn.Module):\n",
    "   \n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        \n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMUR2HB6z_xc"
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNrmnoLz0Awv",
    "outputId": "f4987dfd-bb28-452b-c484-36cc8255074c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel(3, 100).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb3c38d3bd34b27a9c6a6ae353b0ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_workers = 0\n",
    "# batch 사이즈 \n",
    "batch_size = 20\n",
    "# validation을 따로 정해주기 \n",
    "valid_size = 0.2\n",
    "\n",
    "# 데이터를 먼저 normalization 정규화 시킨다. \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "\n",
    "# 여기서는 validation을 따로 뽑아준다. \n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# 여기서 다시 샘플링 \n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# 완성된 3가지의 loader \n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# Class 분류하기 \n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wccqxwrh0B3U",
    "outputId": "81583de2-ba0b-4dda-bed7-665e233bac66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#train_transform = transforms.Compose([    \n",
    "  #  transforms.ToTensor(),\n",
    "  #  transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "#])        \n",
    "\n",
    "#test_transform = transforms.Compose([\n",
    "    #transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "#])    \n",
    "\n",
    "#train = torchvision.datasets.CIFAR100(root=\"./\", train=True, download=True, transform=train_transform)\n",
    "#test = torchvision.datasets.CIFAR100(root=\"./\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(train, batch_size=256,\n",
    "                                          # shuffle=True, num_workers=2)\n",
    "#test_loader = torch.utils.data.DataLoader(test, batch_size=256,\n",
    "                                         # shuffle=False, num_workers=2)\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "#criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.834469 \tValidation Loss: 0.373143\n",
      "Validation loss decreased (inf --> 0.373143).  Saving model ...\n",
      "Epoch: 1 \tTraining Loss: 1.365755 \tValidation Loss: 0.317855\n",
      "Validation loss decreased (0.373143 --> 0.317855).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.227948 \tValidation Loss: 0.292450\n",
      "Validation loss decreased (0.317855 --> 0.292450).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.139733 \tValidation Loss: 0.284260\n",
      "Validation loss decreased (0.292450 --> 0.284260).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.072112 \tValidation Loss: 0.264994\n",
      "Validation loss decreased (0.284260 --> 0.264994).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.014079 \tValidation Loss: 0.264806\n",
      "Validation loss decreased (0.264994 --> 0.264806).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.965680 \tValidation Loss: 0.242882\n",
      "Validation loss decreased (0.264806 --> 0.242882).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.924235 \tValidation Loss: 0.242166\n",
      "Validation loss decreased (0.242882 --> 0.242166).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.889313 \tValidation Loss: 0.232580\n",
      "Validation loss decreased (0.242166 --> 0.232580).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.854094 \tValidation Loss: 0.225886\n",
      "Validation loss decreased (0.232580 --> 0.225886).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.822705 \tValidation Loss: 0.221840\n",
      "Validation loss decreased (0.225886 --> 0.221840).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.790981 \tValidation Loss: 0.220841\n",
      "Validation loss decreased (0.221840 --> 0.220841).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.762819 \tValidation Loss: 0.231701\n",
      "Epoch: 13 \tTraining Loss: 0.737250 \tValidation Loss: 0.212187\n",
      "Validation loss decreased (0.220841 --> 0.212187).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.711238 \tValidation Loss: 0.224342\n",
      "Epoch: 15 \tTraining Loss: 0.687043 \tValidation Loss: 0.213365\n",
      "Epoch: 16 \tTraining Loss: 0.664803 \tValidation Loss: 0.216388\n",
      "Epoch: 17 \tTraining Loss: 0.641551 \tValidation Loss: 0.214348\n",
      "Epoch: 18 \tTraining Loss: 0.622012 \tValidation Loss: 0.212243\n",
      "Epoch: 19 \tTraining Loss: 0.603809 \tValidation Loss: 0.227386\n",
      "Epoch: 20 \tTraining Loss: 0.584218 \tValidation Loss: 0.216669\n",
      "Epoch: 21 \tTraining Loss: 0.564077 \tValidation Loss: 0.222395\n",
      "Epoch: 22 \tTraining Loss: 0.542876 \tValidation Loss: 0.230049\n",
      "Epoch: 23 \tTraining Loss: 0.526450 \tValidation Loss: 0.223348\n",
      "Epoch: 24 \tTraining Loss: 0.507814 \tValidation Loss: 0.240027\n",
      "Epoch: 25 \tTraining Loss: 0.492700 \tValidation Loss: 0.237274\n",
      "Epoch: 26 \tTraining Loss: 0.473457 \tValidation Loss: 0.238562\n",
      "Epoch: 27 \tTraining Loss: 0.458390 \tValidation Loss: 0.265654\n",
      "Epoch: 28 \tTraining Loss: 0.441402 \tValidation Loss: 0.249422\n",
      "Epoch: 29 \tTraining Loss: 0.427589 \tValidation Loss: 0.260853\n",
      "Epoch: 30 \tTraining Loss: 0.413091 \tValidation Loss: 0.263752\n",
      "Epoch: 31 \tTraining Loss: 0.397145 \tValidation Loss: 0.272247\n",
      "Epoch: 32 \tTraining Loss: 0.381265 \tValidation Loss: 0.280982\n",
      "Epoch: 33 \tTraining Loss: 0.368462 \tValidation Loss: 0.284475\n",
      "Epoch: 34 \tTraining Loss: 0.358815 \tValidation Loss: 0.283246\n",
      "Epoch: 35 \tTraining Loss: 0.343838 \tValidation Loss: 0.290734\n",
      "Epoch: 36 \tTraining Loss: 0.332521 \tValidation Loss: 0.304572\n",
      "Epoch: 37 \tTraining Loss: 0.320392 \tValidation Loss: 0.308729\n",
      "Epoch: 38 \tTraining Loss: 0.310448 \tValidation Loss: 0.327814\n",
      "Epoch: 39 \tTraining Loss: 0.297591 \tValidation Loss: 0.321245\n",
      "Epoch: 40 \tTraining Loss: 0.290673 \tValidation Loss: 0.344856\n",
      "Epoch: 41 \tTraining Loss: 0.279071 \tValidation Loss: 0.343371\n",
      "Epoch: 42 \tTraining Loss: 0.272476 \tValidation Loss: 0.353066\n",
      "Epoch: 43 \tTraining Loss: 0.258556 \tValidation Loss: 0.375906\n",
      "Epoch: 44 \tTraining Loss: 0.249377 \tValidation Loss: 0.381714\n",
      "Epoch: 45 \tTraining Loss: 0.243093 \tValidation Loss: 0.377147\n",
      "Epoch: 46 \tTraining Loss: 0.233573 \tValidation Loss: 0.403674\n",
      "Epoch: 47 \tTraining Loss: 0.234027 \tValidation Loss: 0.394555\n",
      "Epoch: 48 \tTraining Loss: 0.225385 \tValidation Loss: 0.405687\n",
      "Epoch: 49 \tTraining Loss: 0.220550 \tValidation Loss: 0.435232\n",
      "Epoch: 50 \tTraining Loss: 0.215400 \tValidation Loss: 0.437297\n",
      "Epoch: 51 \tTraining Loss: 0.204516 \tValidation Loss: 0.437289\n",
      "Epoch: 52 \tTraining Loss: 0.194620 \tValidation Loss: 0.471627\n",
      "Epoch: 53 \tTraining Loss: 0.200268 \tValidation Loss: 0.454130\n",
      "Epoch: 54 \tTraining Loss: 0.186565 \tValidation Loss: 0.483185\n",
      "Epoch: 55 \tTraining Loss: 0.187866 \tValidation Loss: 0.472617\n",
      "Epoch: 56 \tTraining Loss: 0.181540 \tValidation Loss: 0.509153\n",
      "Epoch: 57 \tTraining Loss: 0.181417 \tValidation Loss: 0.506987\n",
      "Epoch: 58 \tTraining Loss: 0.175345 \tValidation Loss: 0.505933\n",
      "Epoch: 59 \tTraining Loss: 0.174181 \tValidation Loss: 0.517091\n",
      "Epoch: 60 \tTraining Loss: 0.170358 \tValidation Loss: 0.524893\n",
      "Epoch: 61 \tTraining Loss: 0.152409 \tValidation Loss: 0.537626\n",
      "Epoch: 62 \tTraining Loss: 0.159763 \tValidation Loss: 0.542325\n",
      "Epoch: 63 \tTraining Loss: 0.150180 \tValidation Loss: 0.562517\n",
      "Epoch: 64 \tTraining Loss: 0.161335 \tValidation Loss: 0.561190\n",
      "Epoch: 65 \tTraining Loss: 0.147421 \tValidation Loss: 0.561374\n",
      "Epoch: 66 \tTraining Loss: 0.140721 \tValidation Loss: 0.597810\n",
      "Epoch: 67 \tTraining Loss: 0.152079 \tValidation Loss: 0.591867\n",
      "Epoch: 68 \tTraining Loss: 0.150000 \tValidation Loss: 0.599915\n",
      "Epoch: 69 \tTraining Loss: 0.150537 \tValidation Loss: 0.600581\n",
      "Epoch: 70 \tTraining Loss: 0.135442 \tValidation Loss: 0.599565\n",
      "Epoch: 71 \tTraining Loss: 0.146124 \tValidation Loss: 0.619394\n",
      "Epoch: 72 \tTraining Loss: 0.150571 \tValidation Loss: 0.614418\n",
      "Epoch: 73 \tTraining Loss: 0.132609 \tValidation Loss: 0.631448\n",
      "Epoch: 74 \tTraining Loss: 0.123827 \tValidation Loss: 0.656580\n",
      "Epoch: 75 \tTraining Loss: 0.134507 \tValidation Loss: 0.646396\n",
      "Epoch: 76 \tTraining Loss: 0.126219 \tValidation Loss: 0.662251\n",
      "Epoch: 77 \tTraining Loss: 0.125336 \tValidation Loss: 0.676540\n",
      "Epoch: 78 \tTraining Loss: 0.129730 \tValidation Loss: 0.662538\n",
      "Epoch: 79 \tTraining Loss: 0.126136 \tValidation Loss: 0.682772\n",
      "Epoch: 80 \tTraining Loss: 0.118593 \tValidation Loss: 0.690486\n",
      "Epoch: 81 \tTraining Loss: 0.124458 \tValidation Loss: 0.680745\n",
      "Epoch: 82 \tTraining Loss: 0.136122 \tValidation Loss: 0.705251\n",
      "Epoch: 83 \tTraining Loss: 0.114546 \tValidation Loss: 0.704243\n",
      "Epoch: 84 \tTraining Loss: 0.100960 \tValidation Loss: 0.736434\n",
      "Epoch: 85 \tTraining Loss: 0.122565 \tValidation Loss: 0.719727\n",
      "Epoch: 86 \tTraining Loss: 0.111162 \tValidation Loss: 0.719590\n",
      "Epoch: 87 \tTraining Loss: 0.103523 \tValidation Loss: 0.742211\n",
      "Epoch: 88 \tTraining Loss: 0.088169 \tValidation Loss: 0.748119\n",
      "Epoch: 89 \tTraining Loss: 0.123476 \tValidation Loss: 0.752322\n",
      "Epoch: 90 \tTraining Loss: 0.120872 \tValidation Loss: 0.744867\n",
      "Epoch: 91 \tTraining Loss: 0.097666 \tValidation Loss: 0.750533\n",
      "Epoch: 92 \tTraining Loss: 0.093856 \tValidation Loss: 0.779288\n",
      "Epoch: 93 \tTraining Loss: 0.111836 \tValidation Loss: 0.779165\n",
      "Epoch: 94 \tTraining Loss: 0.108507 \tValidation Loss: 0.778211\n",
      "Epoch: 95 \tTraining Loss: 0.120335 \tValidation Loss: 0.776157\n",
      "Epoch: 96 \tTraining Loss: 0.117855 \tValidation Loss: 0.767456\n",
      "Epoch: 97 \tTraining Loss: 0.102922 \tValidation Loss: 0.797926\n",
      "Epoch: 98 \tTraining Loss: 0.100423 \tValidation Loss: 0.788293\n",
      "Epoch: 99 \tTraining Loss: 0.095869 \tValidation Loss: 0.805172\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-90dd1f898872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_cifar.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mvalid_loss_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losslist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# train set에 대하여 \n",
    "# number of epochs to train the model\n",
    "n_epochs = [*range(30)]\n",
    "#List to store loss to visualize\n",
    "train_losslist = []\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # tensor를 gpu 로 가능하다면!\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        # batch에 대한 Loss 계산하기 \n",
    "        loss = criterion(output, target)\n",
    "        # parameter에대한 loss backward\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        output = model(data)\n",
    "       \n",
    "        loss = criterion(output, target)\n",
    "        # val loss 계산\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # 평균적인 Loss 계산\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    train_losslist.append(train_loss)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "plt.plot(n_epochs, train_losslist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Performance of Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_cifar.pt'))\n",
    "# 모델저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.060899\n",
      "\n",
      "Test Accuracy of airplane: 62% (624/1000)\n",
      "Test Accuracy of automobile: 77% (777/1000)\n",
      "Test Accuracy of  bird: 54% (545/1000)\n",
      "Test Accuracy of   cat: 42% (424/1000)\n",
      "Test Accuracy of  deer: 53% (534/1000)\n",
      "Test Accuracy of   dog: 47% (471/1000)\n",
      "Test Accuracy of  frog: 70% (709/1000)\n",
      "Test Accuracy of horse: 73% (736/1000)\n",
      "Test Accuracy of  ship: 75% (759/1000)\n",
      "Test Accuracy of truck: 74% (745/1000)\n",
      "\n",
      "Test Accuracy (Overall): 63% (6324/10000)\n"
     ]
    }
   ],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('test performance' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ajWcg7oo0DCD",
    "outputId": "c63fe0c2-3026-4227-a48f-2a98f7508576"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cad004fe989bd74e5c5aec7ffae436e77866a70d082a239967f7d52eef71aa88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
